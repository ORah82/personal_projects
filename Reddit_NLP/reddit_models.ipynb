{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:00.665764Z",
     "start_time": "2019-04-15T20:54:59.380585Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "pd.set_option(\"display.max_colwidth\", 1)\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer ,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import nltk\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:00.673832Z",
     "start_time": "2019-04-15T20:55:00.667988Z"
    }
   },
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:00.698881Z",
     "start_time": "2019-04-15T20:55:00.675739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't know where else to ask, looking for James Urbaniak clip.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want to hear everyone's theories about that scary bear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why isn’t S7 part of the [as] app marathon or Hulu?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some thoughts about if \"everything has a soul\" conversation between henchmen.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the Monarch and Dr. Venture (w/ Spoilers) Theory</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           title  \\\n",
       "0  Don't know where else to ask, looking for James Urbaniak clip.                  \n",
       "1  I want to hear everyone's theories about that scary bear                        \n",
       "2  Why isn’t S7 part of the [as] app marathon or Hulu?                             \n",
       "3  Some thoughts about if \"everything has a soul\" conversation between henchmen.   \n",
       "4  On the Monarch and Dr. Venture (w/ Spoilers) Theory                             \n",
       "\n",
       "   subreddit  \n",
       "0  1          \n",
       "1  1          \n",
       "2  1          \n",
       "3  1          \n",
       "4  1          "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = pd.read_csv('./titles.csv').drop(columns=\"Unnamed: 0\")\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:00.973703Z",
     "start_time": "2019-04-15T20:55:00.970931Z"
    }
   },
   "outputs": [],
   "source": [
    "X_titles = titles['title']\n",
    "\n",
    "y = titles['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep\n",
    "### Tf-idif and Logistic Regresstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:02.248352Z",
     "start_time": "2019-04-15T20:55:02.240071Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titles, y,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:02.752376Z",
     "start_time": "2019-04-15T20:55:02.749239Z"
    }
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:06.088580Z",
     "start_time": "2019-04-15T20:55:03.609857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tvec', 'lr']\n",
      "parameters:\n",
      "{'tvec__max_features': [2500], 'tvec__min_df': [2], 'tvec__max_df': [0.9], 'tvec__ngram_range': [(1, 1), (1, 2)]}\n",
      "done in 2.471s\n",
      "\n",
      "Best score: 0.854\n",
      "Best parameters set:\n",
      "\ttvec__max_df: 0.9\n",
      "\ttvec__max_features: 2500\n",
      "\ttvec__min_df: 2\n",
      "\ttvec__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "pipe_2 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_2_params = {\n",
    "    'tvec__max_features': [2500],\n",
    "    'tvec__min_df': [2],\n",
    "    'tvec__max_df': [.9],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe_2, pipe_2_params,n_jobs=-2, cv=5)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipe_2.steps])\n",
    "print(\"parameters:\")\n",
    "print(pipe_2_params)\n",
    "t0 = time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = gs.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_2_params.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:06.262574Z",
     "start_time": "2019-04-15T20:55:06.176216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8643897538925164"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)\n",
    "\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:09.486408Z",
     "start_time": "2019-04-15T20:55:09.476451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'American Dad'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dict(zip(gs.best_estimator_.named_steps['tvec'].get_feature_names(), \n",
    "             gs.best_estimator_.named_steps['lr'].coef_[0] ))\n",
    "\n",
    "sorted_d = sorted(x.items(), key=lambda x: x[1])\n",
    "#Venture bros: 'I know that after the latest season, with the P.R.O.B.L.E.M. and all, it makes more sense when Action Man says “That’s Entmann, first of Team Venture to go” but when he pisses on his grave says “Venture tradition son!” I’m wondering how you can already HAVE a tradition if nobody has done it before.\n",
    "# Maybe a “rule” or an “S.O.P.” Or something sounds better, I just don’t know about the use of “tradition”.\n",
    "# I know, splitting hairs here but that’s what the internet is for!!! Hahaha.\n",
    "# I just can’t imaging the meeting: “Okay guys. Once one of us dies, THE TRADITION IS GOING TO BE...” just the wording doesn’t fit to me.\n",
    "# Love this fucking show. It’s my absolute #1 fav show ever.'\n",
    "\n",
    "def find_word(x):\n",
    "    test_post = [x]\n",
    "    if gs.predict(test_post) == 1:\n",
    "        return 'Venture Bros'\n",
    "    else:\n",
    "        return 'American Dad'\n",
    "\n",
    "find_word(\"Don't you judge me. Do you know how hard it is to cook for this family? Not very, but I can't handle much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:11.004584Z",
     "start_time": "2019-04-15T20:55:10.993537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01</th>\n",
       "      <td>-0.332259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>010</th>\n",
       "      <td>-2.060440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>-0.740416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02닷컴</th>\n",
       "      <td>-0.337531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03</th>\n",
       "      <td>-1.105497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "01   -0.332259\n",
       "010  -2.060440\n",
       "02   -0.740416\n",
       "02닷컴 -0.337531\n",
       "03   -1.105497"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_coef = gs.best_estimator_.named_steps['lr'].coef_\n",
    "data_coef\n",
    "\n",
    "data_index = gs.best_estimator_.named_steps['tvec'].get_feature_names()\n",
    "\n",
    "df_coef = pd.DataFrame(data_coef, columns = data_index).T\n",
    "df_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T20:55:11.933408Z",
     "start_time": "2019-04-15T20:55:11.929310Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_coef[0].apply(lambda x : np.exp(x)/100)\n",
    "\n",
    "graphme = df_coef[0].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:02:56.672556Z",
     "start_time": "2019-04-15T20:55:13.946743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85451197053407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': None,\n",
       " 'rf__max_features': 4,\n",
       " 'rf__n_estimators': 150,\n",
       " 'tvec__max_df': 0.9,\n",
       " 'tvec__max_features': 3500,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_3 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words=('english', 'episodes'))),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_3_params = {\n",
    "    'rf__n_estimators' : [150],\n",
    "    'rf__max_depth' : [None, 3,4,5],\n",
    "    #'max_leaf_nodes' : [10]\n",
    "    'rf__max_features' : ['auto', 4, 5],\n",
    "    'tvec__max_features': [2500, 3000, 3500],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [.9, .95],\n",
    "    'tvec__ngram_range': [(1,3), (1,2), (1,1)]\n",
    "     \n",
    "}\n",
    "\n",
    "ga= GridSearchCV(pipe_3, param_grid=pipe_3_params, cv=5, n_jobs= -2)\n",
    "ga.fit(X_train, y_train)\n",
    "print(ga.best_score_) #cross val score\n",
    "ga.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:09:43.259017Z",
     "start_time": "2019-04-15T21:05:42.597124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8176795580110497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ada__learning_rate': 0.9,\n",
       " 'ada__n_estimators': 100,\n",
       " 'vect__max_df': 0.75,\n",
       " 'vect__max_features': None,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_4 = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=('english', 'episodes'), tokenizer=LemmaTokenizer())),\n",
    "    ('ada',  AdaBoostClassifier(base_estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "pipe_4_parmas = {\n",
    "    \n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None,750, 1000, 2000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'ada__n_estimators': [100, 150],\n",
    "#     'ada__base_estimator__max_depth': [1,2], [1,1]\n",
    "    'ada__learning_rate': [.9, 1.]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_4,n_jobs=-2, param_grid=pipe_4_parmas, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:09:49.673859Z",
     "start_time": "2019-04-15T21:09:43.261359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8448016072325465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_5 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(tokenizer=LemmaTokenizer())),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_5_parmas = {\n",
    "    'nb__alpha' : (2, 5, 10)\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(pipe_5,pipe_5_parmas,n_jobs=-2, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
